# PAT = Political analysis (of) Tweets - this file holds all methods for processing the ML on a csv file generated by main
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix
from xgboost import XGBClassifier
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction import text
from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix
import mplcursors
import mpld3
from mpld3 import plugins
from matplotlib.lines import Line2D


def cleanMasterCSV(masterCSV):
    textLengths = []
    masterDf = pd.read_csv(masterCSV)
    if (masterDf['subreddit'][0]) == "republicans" or (masterDf['subreddit'][0]) == "the_donald" or (
    masterDf['subreddit'][0]) == "Conservative" or (masterDf['subreddit'][0]) == "socialism":
        masterDf['subreddit'] = masterDf['subreddit'].map(lambda x: 1 if x == "democrats" or x == "socialism" else 0)
    else:
        masterDf['subreddit'] = masterDf['subreddit'].map(
            lambda x: 3 if x == "AuthoritariansDiscuss" or x == "Authoritarianism" else 2)

    masterDf = masterDf.drop(['selftext', 'link_flair_text'], axis=1).rename(columns={"title": "fullText"})
    for item in masterDf["fullText"]:
        textLengths.append(len(item))
    masterDf["textLength"] = textLengths

    print("CSV cleaned and ready for training")
    return masterDf


def baselines(masterDfCleaned):
    X = masterDfCleaned[['fullText', 'textLength']]
    y = masterDfCleaned['subreddit']
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)
    print("Baselines determined:")
    print(y_test.value_counts(normalize=True))


def textClassifierTest(masterDfCleaned, stopWords):
    X = masterDfCleaned[['fullText', 'textLength']]
    y = masterDfCleaned['subreddit']
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

    pipe_cvec_lr = Pipeline([
        ('cvec', CountVectorizer()),
        ('lr', LogisticRegression())
    ])

    pipe_params_cvec_lr = {
        'cvec__max_features': [None, 500, 1000],
        'cvec__min_df': [2, 3],
        'cvec__max_df': [.3, .4, ],
        'cvec__ngram_range': [(1, 2), (1, 3)],
        'cvec__stop_words': [None, 'english', stopWords],
        'lr__penalty': ['l2']
    }

    gs = GridSearchCV(pipe_cvec_lr, param_grid=pipe_params_cvec_lr, cv=5, n_jobs=-1, verbose=1)

    gs.fit(X_train['fullText'], y_train)
    cvlr_bestscore = gs.best_score_
    cvlr_params = gs.best_params_
    cvlr_train = gs.score(X_train["fullText"], y_train)
    cvlr_test = gs.score(X_test["fullText"], y_test)
    cvlr = ('CountVec with LogReg', cvlr_bestscore, cvlr_params, cvlr_train, cvlr_test)
    print(f'Best CV Score: {gs.best_score_}')
    print(f'Best Parameters: {gs.best_params_}')
    print(f'Train Accuracy Score: {gs.score(X_train["fullText"], y_train)}')
    print(f'Test Accuracy Score: {gs.score(X_test["fullText"], y_test)}')


def textLRCVtest(masterDfCleaned, stopWords, type):
    # define features
    X = masterDfCleaned['fullText']
    y = masterDfCleaned['subreddit']

    # train test split
    X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y,
                                                        stratify=y,
                                                        random_state=42)

    # instantiate countvectorizer
    cvec = CountVectorizer(stop_words=stopWords,
                           ngram_range=(1, 2), min_df=2,
                           max_features=None, max_df=0.4)
    # Fit our CountVectorizer on the training data and transform training data.
    X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train).todense()
                                , columns=cvec.get_feature_names())

    # Fit our CountVectorizer on the test data and transform training data.
    X_test_cvec = pd.DataFrame(cvec.transform(X_test).todense()
                               , columns=cvec.get_feature_names())

    # instantiate logisticregression
    lr = LogisticRegression()
    # fit data
    lr = lr.fit(X_train_cvec, y_train)

    # score our logistic regression model on our fitted training data
    print("Training Score:")
    print(lr.score(X_train_cvec, y_train))

    # score our logistic regression model on our fitted testing data
    print("Testing Score:")
    print(lr.score(X_test_cvec, y_test))

    # generate predictions
    pred = lr.predict(X_test_cvec)

    # generate confusion matrix
    conf = confusion_matrix(y_test,  # True values.
                            pred)  # Predicted values.
    tn, fp, fn, tp = conf.ravel()

    if type == "AuthLib":
        x = "Libertarian"
        y = "Authoritarian"
    else:
        x = "republican"
        y = "democrats"
    # convert confusion matrix to dataframe
    df_lr = pd.DataFrame(conf, index=['actual ' + x, 'actual ' + y],
                         columns=['predicted ' + x, 'predicted ' + y])
    print(df_lr)

    params = lr.get_params()
    # print("Final Paramerters:")
    # print(params)

    return lr, cvec


def lrPredictor(lr, cvec, testData):
    print("Testing the sentence: \"" + testData[0] + "\"")
    dictMap = {0: "Republican", 1: "Democrat", 2: "Libertarian", 3: "Authoritarian"}
    X_test_cvec = pd.DataFrame(cvec.transform(testData).todense()
                               , columns=cvec.get_feature_names_out())

    prediction = lr.predict(X_test_cvec)
    probability = lr.predict_proba(X_test_cvec)
    print(probability)
    print(prediction)
    print("Predicted Political Alignment: " + dictMap.get(prediction[0]))
    if prediction > 1:
        prediction -= 2
    confidence = probability[0][prediction[0]]
    print("Confidence: ", confidence)
    return prediction, probability


def compassPredictions(lr0, cvec0, lr1, cvec1, testPhrases):
    fig = plt.figure(figsize=(14, 14), dpi=80)
    plt.plot([.5, .5], [1, 0], 'k-', lw=2)
    plt.plot([0, 1], [.5, .5], 'k-', lw=2)
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.title("Political Alignment of Boris Johnson's last 100 tweets")
    plt.xlabel("Economic Left/Right")
    plt.ylabel("Economic Libertarian/Authoritarian")
    plt.grid()
    xpoints = []
    ypoints = []
    labels = []
    for phrase in testPhrases:
        x = ((lrPredictor(lr0, cvec0, [phrase]))[1][0][0])
        y = ((lrPredictor(lr1, cvec1, [phrase]))[1][0][1])
        if (0.01 < x < 0.99) and (0.01 < y < 0.99):
            xpoints.append(x)
            ypoints.append(y)
            labels.append(phrase)
            (plt.scatter(x, y, label=phrase, c="blue"))
    points = plt.scatter(xpoints, ypoints)
    print(len(labels))
    mplcursors.cursor()
    # plt.legend( loc = 'upper center', bbox_to_anchor = (0.5, 0.5),ncol=1,fontsize='x-small')

    print(points)
    tooltip = plugins.PointHTMLTooltip(points, labels,
                                       voffset=10, hoffset=10)
    plugins.connect(fig, tooltip)
    html_str = mpld3.fig_to_html(fig)
    Html_file = open("indexBoris.html", "w")
    Html_file.write(html_str)
    Html_file.close()
    mpld3.show()
    plt.show()


def compassPrediction(lr0, cvec0, lr1, cvec1, testPhrase):
    x = ((lrPredictor(lr0, cvec0, [testPhrase]))[1][0][0])
    y = ((lrPredictor(lr1, cvec1, [testPhrase]))[1][0][1])
    plt.plot([.5, .5], [1, 0], 'k-', lw=2)
    plt.plot([0, 1], [.5, .5], 'k-', lw=2)
    plt.scatter(x, y)
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.title("Political distribution for \"" + testPhrase + "\"")
    plt.xlabel("Economic Left/Right")
    plt.ylabel("Economic Libertarian/Authoritarian")
    plt.grid()
    plt.show()


def csvToListOfStringsTrump(csv):
    df = pd.read_csv(csv)
    df = df.drop(["id", "link", "date", "retweets", "favorites", "mentions", "hashtags"], axis=1)
    N = 43000
    df = df.iloc[N:, :]
    listOfTweets = []
    for i in df['content']:
        if "https" not in i and "twitter" not in i:
            listOfTweets.append(i)
    return listOfTweets


def csvToListOfStringsBoris(csv):
    df = pd.read_csv(csv)
    N = 0
    df = df.iloc[N:, :]
    listOfTweets = []
    for i in df['content']:
        if "https" not in i and "twitter" not in i and "de" not in i and "avec" not in i and "le" not in i and "_" not in i:
            listOfTweets.append(i)
    return listOfTweets


def compassPredictionsCombined(lr0, cvec0, lr1, cvec1, testPhrasesDict):
    fig = plt.figure(figsize=(14, 14), dpi=80)
    plt.plot([.5, .5], [1, 0], 'k-', lw=2)
    plt.plot([0, 1], [.5, .5], 'k-', lw=2)
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.title("Political Alignment of 5 Country Leader's Last 100 Tweets")
    plt.xlabel("Economic Left/Right")
    plt.ylabel("Economic Libertarian/Authoritarian")
    plt.grid()
    xpoints = []
    ypoints = []
    labels = []

    testPhrases = testPhrasesDict["Boris"]
    n = 0
    for phrase in testPhrases:
        if n <= 100:
            x = ((lrPredictor(lr0, cvec0, [phrase]))[1][0][0])
            y = ((lrPredictor(lr1, cvec1, [phrase]))[1][0][1])
            if (0.01 < x < 0.99) and (0.01 < y < 0.99):
                xpoints.append(x)
                ypoints.append(y)
                labels.append(phrase)
                (plt.scatter(x, y, label=phrase, c="blue"))
            n += 1

    x1points = []
    y1points = []
    labels1 = []
    testPhrases = testPhrasesDict["Trump"]
    n = 0
    for phrase in testPhrases:
        if n <= 100:
            x = ((lrPredictor(lr0, cvec0, [phrase]))[1][0][0])
            y = ((lrPredictor(lr1, cvec1, [phrase]))[1][0][1])
            if (0.01 < x < 0.99) and (0.01 < y < 0.99):
                x1points.append(x)
                y1points.append(y)
                labels1.append(phrase)
                (plt.scatter(x, y, label=phrase, c="red"))
            n += 1
    x2points = []
    y2points = []
    labels2 = []
    testPhrases = testPhrasesDict["Lee"]
    n = 0
    for phrase in testPhrases:
        if n <= 100:
            x = ((lrPredictor(lr0, cvec0, [phrase]))[1][0][0])
            y = ((lrPredictor(lr1, cvec1, [phrase]))[1][0][1])
            if (0.01 < x < 0.99) and (0.01 < y < 0.99):
                x2points.append(x)
                y2points.append(y)
                labels2.append(phrase)
                (plt.scatter(x, y, label=phrase, c="g"))
            n += 1
    x3points = []
    y3points = []
    labels3 = []
    testPhrases = testPhrasesDict["Trudeau"]
    n = 0
    for phrase in testPhrases:
        if n <= 100:
            x = ((lrPredictor(lr0, cvec0, [phrase]))[1][0][0])
            y = ((lrPredictor(lr1, cvec1, [phrase]))[1][0][1])
            if (0.01 < x < 0.99) and (0.01 < y < 0.99):
                x3points.append(x)
                y3points.append(y)
                labels3.append(phrase)
                (plt.scatter(x, y, label=phrase, c="y"))
            n += 1

    x4points = []
    y4points = []
    labels4 = []
    testPhrases = testPhrasesDict["Modi"]
    n = 0
    for phrase in testPhrases:
        if n <= 100:
            x = ((lrPredictor(lr0, cvec0, [phrase]))[1][0][0])
            y = ((lrPredictor(lr1, cvec1, [phrase]))[1][0][1])
            if (0.01 < x < 0.99) and (0.01 < y < 0.99):
                x4points.append(x)
                y4points.append(y)
                labels4.append(phrase)
                (plt.scatter(x, y, label=phrase, c="darkslategrey"))
            n += 1

    legend_elements = [
        Line2D([0], [0], marker='o', color='b', label='Boris Johnson', markerfacecolor='b', markersize=5),
        Line2D([0], [0], marker='o', color='r', label='Donald Trump', markerfacecolor='r', markersize=5),
        Line2D([0], [0], marker='o', color='g', label='Lee Hsien Loong', markerfacecolor='g', markersize=5),
        Line2D([0], [0], marker='o', color='y', label='Justin Trudeau', markerfacecolor='y', markersize=5),
        Line2D([0], [0], marker='o', color='darkslategrey', label='Narendra Modi', markerfacecolor='darkslategrey', markersize=5)
    ]

    ax = plt.subplot()
    ax.legend(handles=legend_elements)
    points = plt.scatter(xpoints, ypoints, c='b')
    points1 = plt.scatter(x1points, y1points, c='r')
    points2 = plt.scatter(x2points, y2points, c='g')
    points3 = plt.scatter(x3points, y3points, c='y')
    points4 = plt.scatter(x4points, y4points, c='darkslategrey')
    print(len(labels))
    mplcursors.cursor()

    tooltip = plugins.PointHTMLTooltip(points, labels,
                                       voffset=10, hoffset=10)
    plugins.connect(fig, tooltip)
    tooltip = plugins.PointHTMLTooltip(points1, labels1,
                                       voffset=10, hoffset=10)
    plugins.connect(fig, tooltip)
    tooltip = plugins.PointHTMLTooltip(points2, labels2,
                                       voffset=10, hoffset=10)
    plugins.connect(fig, tooltip)
    tooltip = plugins.PointHTMLTooltip(points3, labels3,
                                       voffset=10, hoffset=10)
    plugins.connect(fig, tooltip)
    tooltip = plugins.PointHTMLTooltip(points4, labels4,
                                       voffset=10, hoffset=10)
    plugins.connect(fig, tooltip)
    html_str = mpld3.fig_to_html(fig)
    Html_file = open("indexCombined.html", "w")
    Html_file.write(html_str)
    Html_file.close()
    mpld3.show()
    plt.show()


def MLmain():
    additionalStopWords = ['www', 'things', 'does', 'x200b', 'amp',
                           'just', 'like', 'https', 'com', 'watch', 'want',
                           'says', 'say', 'did', 'this']

    stopWords = text.ENGLISH_STOP_WORDS.union(additionalStopWords)

    print("Cleaning Provided CSV for Learning...")
    masterDfCleanedDemRep = cleanMasterCSV('./data/masterDF.csv')
    masterDfCleanedAuthLib = cleanMasterCSV('./data/masterDFAuthLib.csv')
    print("Setting Parameters for Training and Determining Baselines...")
    baselines(masterDfCleanedDemRep)
    baselines(masterDfCleanedAuthLib)

    print("Attempting Text Classification With LogisticRegression and CountVectorizer...")
    lr0, cvec0 = textLRCVtest(masterDfCleanedDemRep, stopWords, "DemRep")
    lr1, cvec1 = textLRCVtest(masterDfCleanedAuthLib, stopWords, "AuthLib")

    # compassPredictions(lr0,cvec0,lr1,cvec1,csvToListOfStringsTrump('./data/realdonaldtrump.csv'))
    # compassPredictions(lr0,cvec0,lr1,cvec1,csvToListOfStringsBoris('./data/borisjohnson.csv'))

    testPhrasesDict = {}
    testPhrasesDict['Boris'] = csvToListOfStringsBoris('./data/borisjohnson.csv')
    testPhrasesDict['Trump'] = csvToListOfStringsTrump('./data/realdonaldtrump.csv')
    testPhrasesDict['Lee'] = csvToListOfStringsBoris('./data/leeHseinLoong.csv')
    testPhrasesDict['Trudeau'] = csvToListOfStringsBoris('./data/justinTrudeau.csv')
    testPhrasesDict['Modi'] = csvToListOfStringsBoris('./data/narendraModi.csv')
    compassPredictionsCombined(lr0, cvec0, lr1, cvec1, testPhrasesDict)


if __name__ == "__main__":
    MLmain()

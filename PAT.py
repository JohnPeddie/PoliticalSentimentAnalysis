#PAT = Political analysis (of) Tweets - this file holds all methods for processing the ML on a csv file generated by main
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix
from xgboost import XGBClassifier
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction import text
from sklearn.metrics import accuracy_score,recall_score,precision_score, confusion_matrix
import mplcursors
import mpld3
from mpld3 import plugins

def cleanMasterCSV(masterCSV):

    textLengths=[]
    masterDf = pd.read_csv(masterCSV)
    if (masterDf['subreddit'][0])== "republicans":
        masterDf['subreddit'] = masterDf['subreddit'].map(lambda x: 1 if x == "democrats" else 0)
    else:
        masterDf['subreddit'] = masterDf['subreddit'].map(lambda x: 3 if x == "AuthoritariansDiscuss" or x == "Authoritarianism" else 2)

    masterDf = masterDf.drop(['selftext','link_flair_text'],axis=1).rename(columns={"title":"fullText"})
    for item in masterDf["fullText"]:
        textLengths.append(len(item))
    masterDf["textLength"]=textLengths

    print("CSV cleaned and ready for training")
    return masterDf

def baselines(masterDfCleaned):
    X = masterDfCleaned[['fullText', 'textLength']]
    y = masterDfCleaned['subreddit']
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)
    print("Baselines determined:")
    print(y_test.value_counts(normalize=True))

def textClassifierTest(masterDfCleaned,stopWords):
    X = masterDfCleaned[['fullText', 'textLength']]
    y = masterDfCleaned['subreddit']
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)


    pipe_cvec_lr = Pipeline([
        ('cvec', CountVectorizer()),
        ('lr', LogisticRegression())
    ])

    pipe_params_cvec_lr = {
        'cvec__max_features': [None, 500, 1000],
        'cvec__min_df': [2, 3],
        'cvec__max_df': [.3, .4, ],
        'cvec__ngram_range': [(1, 2), (1, 3)],
        'cvec__stop_words': [None, 'english', stopWords],
        'lr__penalty': ['l2']
    }

    gs = GridSearchCV(pipe_cvec_lr, param_grid=pipe_params_cvec_lr, cv=5, n_jobs=-1, verbose=1)

    gs.fit(X_train['fullText'], y_train)
    cvlr_bestscore = gs.best_score_
    cvlr_params = gs.best_params_
    cvlr_train = gs.score(X_train["fullText"], y_train)
    cvlr_test = gs.score(X_test["fullText"], y_test)
    cvlr = ('CountVec with LogReg', cvlr_bestscore, cvlr_params, cvlr_train, cvlr_test)
    print(f'Best CV Score: {gs.best_score_}')
    print(f'Best Parameters: {gs.best_params_}')
    print(f'Train Accuracy Score: {gs.score(X_train["fullText"], y_train)}')
    print(f'Test Accuracy Score: {gs.score(X_test["fullText"], y_test)}')

def textLRCVtest(masterDfCleaned,stopWords,type):
    # define features
    X = masterDfCleaned['fullText']
    y = masterDfCleaned['subreddit']

    # train test split
    X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y,
                                                        stratify=y,
                                                        random_state=42)

    # instantiate countvectorizer
    cvec = CountVectorizer(stop_words=stopWords,
                           ngram_range=(1, 2), min_df=2,
                           max_features=None, max_df=0.4)
    # Fit our CountVectorizer on the training data and transform training data.
    X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train).todense()
                                , columns=cvec.get_feature_names())

    # Fit our CountVectorizer on the test data and transform training data.
    X_test_cvec = pd.DataFrame(cvec.transform(X_test).todense()
                               , columns=cvec.get_feature_names())

    # instantiate logisticregression
    lr = LogisticRegression()
    # fit data
    lr = lr.fit(X_train_cvec, y_train)

    # score our logistic regression model on our fitted training data
    print("Training Score:")
    print(lr.score(X_train_cvec, y_train))

    # score our logistic regression model on our fitted testing data
    print("Testing Score:")
    print(lr.score(X_test_cvec, y_test))


    # generate predictions
    pred = lr.predict(X_test_cvec)

    # generate confusion matrix
    conf = confusion_matrix(y_test,  # True values.
                            pred)  # Predicted values.
    tn, fp, fn, tp = conf.ravel()

    if type == "AuthLib":
        x = "Libertarian"
        y = "Authoritarian"
    else:
        x = "republican"
        y = "democrats"
    # convert confusion matrix to dataframe
    df_lr = pd.DataFrame(conf, index=['actual '+x, 'actual '+y],
                         columns=['predicted '+x, 'predicted '+y])
    print(df_lr)

    params = lr.get_params()
    #print("Final Paramerters:")
    #print(params)


    return lr,cvec

def lrPredictor(lr,cvec, testData):
    print("Testing the sentence: \""+ testData[0]+"\"")
    dictMap = {0:"Republican", 1:"Democrat",2:"Libertarian",3:"Authoritarian"}
    X_test_cvec = pd.DataFrame(cvec.transform(testData).todense()
                               , columns=cvec.get_feature_names_out())

    prediction = lr.predict(X_test_cvec)
    probability = lr.predict_proba(X_test_cvec)
    print(probability)
    print(prediction)
    print("Predicted Political Alignment: " + dictMap.get(prediction[0]))
    if prediction > 1:
        prediction -= 2
    confidence = probability[0][prediction[0]]
    print("Confidence: ", confidence)
    return  prediction,probability

def compassPredictions(lr0,cvec0,lr1,cvec1,testPhrases):

    fig = plt.figure(figsize=(14, 14), dpi=80)
    plt.plot([.5, .5], [1, 0], 'k-', lw=2)
    plt.plot([0, 1], [.5, .5], 'k-', lw=2)
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.title("Political Alignment of Donald Trump's last 30000- tweets")
    plt.xlabel("Economic Left/Right")
    plt.ylabel("Economic Libertarian/Authoritarian")
    plt.grid()
    xpoints =[]
    ypoints =[]
    labels =[]
    for phrase in testPhrases:
        x = ((lrPredictor(lr0, cvec0, [phrase]))[1][0][0])
        y = ((lrPredictor(lr1, cvec1, [phrase]))[1][0][1])
        xpoints.append(x)
        ypoints.append(y)
        labels.append(phrase)
        (plt.scatter(x, y,label =phrase))
    points = plt.scatter(xpoints,ypoints,c="red")
    mplcursors.cursor()
    #plt.legend( loc = 'upper center', bbox_to_anchor = (0.5, 0.5),ncol=1,fontsize='x-small')


    print(points)
    tooltip = plugins.PointHTMLTooltip(points, labels,
                                       voffset=10, hoffset=10)
    plugins.connect(fig, tooltip)
    html_str = mpld3.fig_to_html(fig)
    Html_file = open("index.html", "w")
    Html_file.write(html_str)
    Html_file.close()
    mpld3.show()
    plt.show()




def compassPrediction(lr0,cvec0,lr1,cvec1,testPhrase):
    x = ((lrPredictor(lr0, cvec0, [testPhrase]))[1][0][0])
    y = ((lrPredictor(lr1, cvec1, [testPhrase]))[1][0][1])
    plt.plot([.5, .5], [1, 0], 'k-', lw=2)
    plt.plot([0, 1], [.5, .5], 'k-', lw=2)
    plt.scatter(x, y)
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.title("Political distribution for \"" + testPhrase + "\"")
    plt.xlabel("Economic Left/Right")
    plt.ylabel("Economic Libertarian/Authoritarian")
    plt.grid()
    plt.show()

def csvToListOfStrings(csv):
    df = pd.read_csv(csv)
    df = df.drop(["id","link","date","retweets","favorites","mentions","hashtags"],axis= 1)
    N = 300
    df = df.iloc[N:, :]
    listOfTweets =[]
    for i in df['content']:
        if "https" not in i and "twitter" not in i:
            listOfTweets.append(i)
    return listOfTweets




def MLmain():
    additionalStopWords = ['www', 'things', 'does', 'x200b', 'amp',
                               'just', 'like', 'https', 'com', 'watch', 'want',
                               'says', 'say', 'did', 'this']

    stopWords = text.ENGLISH_STOP_WORDS.union(additionalStopWords)

    print("Cleaning Provided CSV for Learning...")
    masterDfCleanedDemRep = cleanMasterCSV('./data/masterDF.csv')
    masterDfCleanedAuthLib = cleanMasterCSV('./data/masterDFAuthLib.csv')
    print("Setting Parameters for Training and Determining Baselines...")
    baselines(masterDfCleanedDemRep)
    baselines(masterDfCleanedAuthLib)

    print("Attempting Text Classification With LogisticRegression and CountVectorizer...")
    lr0,cvec0 = textLRCVtest(masterDfCleanedDemRep,stopWords,"DemRep")
    lr1,cvec1 = textLRCVtest(masterDfCleanedAuthLib,stopWords,"AuthLib")

    compassPredictions(lr0,cvec0,lr1,cvec1,csvToListOfStrings('./data/realdonaldtrump.csv'))





if __name__ == "__main__":
    MLmain()